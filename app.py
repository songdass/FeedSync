import re
from collections import Counter
from datetime import datetime, timedelta, timezone
from functools import lru_cache
from typing import Dict, List, Tuple

import altair as alt
import feedparser
import pandas as pd
import streamlit as st
from dateutil import parser as date_parser


st.set_page_config(
    page_title="í•œí™” ë‰´ìŠ¤ íŠ¸ë Œë“œ ëŒ€ì‹œë³´ë“œ",
    page_icon="ğŸ“°",
    layout="wide",
)


LANGUAGE_CONFIG = {
    "í•œêµ­ì–´": {"hl": "ko", "gl": "KR", "ceid": "KR:ko", "timezone": timezone(timedelta(hours=9))},
    "English": {"hl": "en", "gl": "US", "ceid": "US:en", "timezone": timezone.utc},
    "æ—¥æœ¬èª": {"hl": "ja", "gl": "JP", "ceid": "JP:ja", "timezone": timezone(timedelta(hours=9))},
}

STOPWORDS = {
    "í•œí™”",
    "ê´€ë ¨",
    "ê¸°ì‚¬",
    "ë³´ë„",
    "ë•Œë¬¸",
    "ëŒ€í•œ",
    "2024",
    "2025",
    "ê¸°ì",
    "ì‚¬ì§„",
    "ì œê³µ",
    "ì†ë³´",
    "ì˜¤ëŠ˜",
    "ì§€ë‚œ",
    "ëŒ€í•œë¯¼êµ­",
    "í•œêµ­",
    "ìµœê·¼",
    "ì—…ê³„",
    "ì´ë²ˆ",
    "ê³ ê°",
    "ê¸°ì—…",
    "ì„ ì •",
    "ë°œí‘œ",
    "ëŒ€í‘œ",
    "ì¶œì‹œ",
    "ì§„í–‰",
}


def build_google_news_rss(query: str, language_key: str) -> Tuple[str, timezone]:
    config = LANGUAGE_CONFIG[language_key]
    base = "https://news.google.com/rss/search"
    params = f"?q={query}&hl={config['hl']}&gl={config['gl']}&ceid={config['ceid']}"
    return f"{base}{params}", config["timezone"]


@st.cache_data(ttl=1800, show_spinner=False)
def load_news(query: str, language_key: str) -> pd.DataFrame:
    rss_url, tz = build_google_news_rss(query, language_key)
    feed = feedparser.parse(rss_url)

    records: List[Dict] = []
    for entry in feed.entries:
        published = _parse_published(entry, tz)
        summary = _clean_html(entry.get("summary", ""))
        source = _extract_source(entry)

        records.append(
            {
                "title": entry.get("title", "").strip(),
                "summary": summary,
                "link": entry.get("link"),
                "source": source,
                "published_at": published,
                "published_date": published.date() if published else None,
            }
        )

    df = pd.DataFrame(records)
    if df.empty:
        return df

    df = df.sort_values(by="published_at", ascending=False).reset_index(drop=True)
    return df


def _parse_published(entry, tz: timezone):
    if "published_parsed" in entry and entry.published_parsed:
        dt = datetime(*entry.published_parsed[:6], tzinfo=timezone.utc)
    elif "published" in entry:
        try:
            dt = date_parser.parse(entry.published)
        except (ValueError, TypeError):
            return None
    else:
        return None

    return dt.astimezone(tz)


def _clean_html(text: str) -> str:
    return re.sub("<[^<]+?>", "", text or "").strip()


def _extract_source(entry) -> str:
    source = entry.get("source")
    if isinstance(source, dict):
        return source.get("title", "").strip()
    if hasattr(source, "title"):
        return source.title.strip()
    return entry.get("author", "").strip()


@lru_cache(maxsize=128)
def extract_keywords(text: str) -> List[str]:
    # Basic tokenization suited for Korean and English mix
    text = re.sub(r"[^\w\s]", " ", text)
    tokens = [token.lower() for token in text.split() if len(token) > 1]
    return [token for token in tokens if token not in STOPWORDS]


def get_keyword_counts(df: pd.DataFrame, max_keywords: int = 20) -> pd.DataFrame:
    if df.empty:
        return pd.DataFrame(columns=["keyword", "count"])

    counter: Counter = Counter()
    for _, row in df.iterrows():
        tokens = extract_keywords(f"{row['title']} {row['summary']}")
        counter.update(tokens)

    most_common = counter.most_common(max_keywords)
    return pd.DataFrame(most_common, columns=["keyword", "count"])


def render_header(query: str, df: pd.DataFrame):
    st.title("ğŸ“° í•œí™” ë‰´ìŠ¤ íŠ¸ë Œë“œ ëŒ€ì‹œë³´ë“œ")
    st.caption("Google News RSS ë°ì´í„°ë¥¼ í™œìš©í•´ ì‹¤ì‹œê°„ìœ¼ë¡œ í•œí™” ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•˜ê³  ì‹œê°í™”í•©ë‹ˆë‹¤.")

    metrics = st.columns(3)
    metrics[0].metric("ìˆ˜ì§‘ëœ ê¸°ì‚¬ ìˆ˜", f"{len(df):,}")

    if not df.empty and df["published_at"].notna().any():
        latest_time = df["published_at"].dropna().max()
        metrics[1].metric("ê°€ì¥ ìµœê·¼ ê¸°ì‚¬", latest_time.strftime("%Y-%m-%d %H:%M"))

        hours = (datetime.now(latest_time.tzinfo) - latest_time).total_seconds() / 3600
        freshness = f"{hours:.1f}ì‹œê°„ ì „"
        metrics[2].metric("ìµœì‹ ì„±", freshness)
    else:
        metrics[1].metric("ê°€ì¥ ìµœê·¼ ê¸°ì‚¬", "-")
        metrics[2].metric("ìµœì‹ ì„±", "-")


def render_trend_charts(df: pd.DataFrame):
    if df.empty:
        st.warning("í‘œì‹œí•  ë‰´ìŠ¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ê²€ìƒ‰ì–´ ë˜ëŠ” ì–¸ì–´ë¥¼ ì¡°ì •í•´ ë³´ì„¸ìš”.")
        return

    with st.container():
        st.subheader("íŠ¸ë Œë“œ ë¶„ì„")
        chart_cols = st.columns((2, 1))

        counts_by_day = (
            df.groupby("published_date")
            .size()
            .reset_index(name="articles")
            .dropna()
        )
        if not counts_by_day.empty:
            timeline_chart = (
                alt.Chart(counts_by_day)
                .mark_area(interpolate="monotone", line=True, point=True)
                .encode(
                    x=alt.X("published_date:T", title="ë‚ ì§œ"),
                    y=alt.Y("articles:Q", title="ê¸°ì‚¬ ìˆ˜"),
                    tooltip=["published_date:T", "articles:Q"],
                )
                .properties(height=260)
            )
            chart_cols[0].altair_chart(timeline_chart, use_container_width=True)
        else:
            chart_cols[0].info("ê¸°ì‚¬ ë‚ ì§œ ì •ë³´ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

        source_counts = (
            df.groupby("source")
            .size()
            .reset_index(name="articles")
            .sort_values("articles", ascending=False)
            .head(10)
        )
        if not source_counts.empty:
            source_chart = (
                alt.Chart(source_counts)
                .mark_bar()
                .encode(
                    x=alt.X("articles:Q", title="ê¸°ì‚¬ ìˆ˜"),
                    y=alt.Y("source:N", sort="-x", title="ì–¸ë¡ ì‚¬"),
                    tooltip=["source:N", "articles:Q"],
                    color=alt.Color(
                        "articles:Q", scale=alt.Scale(scheme="blues"), legend=None
                    ),
                )
                .properties(height=260)
            )
            chart_cols[1].altair_chart(source_chart, use_container_width=True)
        else:
            chart_cols[1].info("ì–¸ë¡ ì‚¬ ì •ë³´ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")


def render_keywords(df: pd.DataFrame):
    keywords = get_keyword_counts(df)
    if keywords.empty:
        st.info("í‚¤ì›Œë“œ í†µê³„ë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆëŠ” ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
        return

    st.subheader("í•µì‹¬ í‚¤ì›Œë“œ")
    keyword_chart = (
        alt.Chart(keywords)
        .mark_bar()
        .encode(
            x=alt.X("count:Q", title="ë¹ˆë„"),
            y=alt.Y("keyword:N", sort="-x", title="í‚¤ì›Œë“œ"),
            tooltip=["keyword:N", "count:Q"],
            color=alt.Color("count:Q", scale=alt.Scale(scheme="orangered"), legend=None),
        )
        .properties(height=400)
    )
    st.altair_chart(keyword_chart, use_container_width=True)


def render_article_feed(df: pd.DataFrame):
    st.subheader("ê¸°ì‚¬ í”¼ë“œ")
    st.caption("ìµœì‹  ê¸°ì‚¬ë¶€í„° ìˆœì°¨ì ìœ¼ë¡œ ìŠ¤í¬ë¡¤ í•˜ë©´ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    if df.empty:
        st.info("í‘œì‹œí•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    for _, row in df.iterrows():
        with st.container():
            st.markdown(f"#### [{row['title']}]({row['link']})")
            meta = []
            if row["source"]:
                meta.append(row["source"])
            if row["published_at"]:
                meta.append(row["published_at"].strftime("%Y-%m-%d %H:%M"))
            st.caption(" Â· ".join(meta))
            if row["summary"]:
                st.write(row["summary"])
            st.divider()


def main():
    st.sidebar.header("ê²€ìƒ‰ ì„¤ì •")
    query = st.sidebar.text_input("ê²€ìƒ‰ì–´", value="í•œí™”")
    language = st.sidebar.selectbox("ì–¸ì–´", options=list(LANGUAGE_CONFIG.keys()), index=0)

    if st.sidebar.button("ë°ì´í„° ìƒˆë¡œê³ ì¹¨"):
        load_news.clear()
        st.experimental_rerun()

    with st.spinner("ë‰´ìŠ¤ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
        df = load_news(query.strip(), language)

    render_header(query, df)
    render_trend_charts(df)
    render_keywords(df)
    render_article_feed(df)


if __name__ == "__main__":
    main()
